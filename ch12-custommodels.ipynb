{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Tensorflow is open-source library for numerical computation.  \n",
    "* It particularly made for large scale machine learning task which support GPU computation.  \n",
    "* Optimization API based on reverse-mode autofidd which is fast to compute gradient and Jacobian.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is not a drop-in replacement, as it have some differences in naming, computing and characteristics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "They are different in datatype, tensorflow default int is int32 while numpy default int is int64."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Tensor\n",
    "* Tensor Variable. \n",
    "* Tensor Array. \n",
    "* Tensor String. \n",
    "* "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Custom loss function should be used if there are no hyperparameters in the function. As it will not be saved along with the model. \n",
    "* And if you use the subclass method, you have to override get_config() method to saved neccessary hyperparmeter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* If the custom metric have to compute something that doesn't equal between the whole epoch and the mean metric over all the batches. You have to use sublass."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Custom layers should be used for the reusable blocks that cannot train by itselves.  \n",
    "* Custom models should be used for trainable objects."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Custom training loops should not be used, if it is not absoultely needed. Because it is error prone and the code will be longer. However, if we want to implement some complex models that comes out from research paper, we have to use custom training loops. We have to  \n",
    "* create our model  \n",
    "* create gradient tape for cost function  \n",
    "* forward the input  \n",
    "* get the gradient  \n",
    "* update the trainable parameters  \n",
    "* print the status  \n",
    "* repeat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Keras custom component should stick to TF operations as much as possible to improve performance and portable to another devices.  \n",
    "* You can use arbitrary python code though, by setting decorating with tf.py_function() or by passing argument dynamic=True in custom layers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 11"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Dynamic model prevent keras to build a graph. This create limitation in portability. Dynamic model only executable if the machine install python.  \n",
    "* Also, the performance is also lower because the pure python code cannot be optimize into a graph."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "## create cutom layers that perform layernormalization\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import LayerNormalization\n",
    "from tensorflow.keras.layers import Layer\n",
    "from tensorflow.keras.models import Sequential\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyLayerNorm(Layer):\n",
    "    '''\n",
    "    init\n",
    "    build\n",
    "    call\n",
    "    \n",
    "    '''\n",
    "    def __init__(self, axis = 0, eps = 0.001, *args, **kwargs):\n",
    "        self.axis = 0\n",
    "        self.eps = eps\n",
    "        super().__init__(*args, **kwargs)\n",
    "        return\n",
    "    \n",
    "    def build(self, input_shape):\n",
    "        self.beta = tf.Variable(tf.zeros(input_shape[-1:]))\n",
    "        self.alpha = tf.Variable(tf.ones(input_shape[-1:]))\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        mean, variance = tf.nn.moments(inputs, axes = 1, keepdims=True,)\n",
    "        print(f'input : {inputs}')\n",
    "        print(f'mean : {mean.shape}')\n",
    "        print(f'variance : {variance.shape}')\n",
    "        print(f'beta : {self.beta.shape}')\n",
    "        print(f'alpha : {self.alpha.shape}')\n",
    "        outputs = (inputs-mean)/(tf.sqrt(variance) + self.eps)\n",
    "        # shift and rescale\n",
    "        outputs = outputs*self.alpha + self.beta\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "float32\n",
      "(2, 10)\n"
     ]
    }
   ],
   "source": [
    "arr = np.arange(20).reshape(2,10).astype('float32')\n",
    "print(arr.dtype)\n",
    "print(arr.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "\n",
    "l1 = LayerNormalization()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "l2 = MyLayerNorm()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 10), dtype=float32, numpy=\n",
       "array([[-1.566604  , -1.2184697 , -0.8703356 , -0.5222013 , -0.17406711,\n",
       "         0.17406711,  0.5222013 ,  0.8703356 ,  1.2184697 ,  1.566604  ],\n",
       "       [-1.566604  , -1.2184697 , -0.8703356 , -0.5222013 , -0.17406711,\n",
       "         0.17406711,  0.5222013 ,  0.8703356 ,  1.2184697 ,  1.566604  ]],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 231,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l1(arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input : [[ 0.  1.  2.  3.  4.  5.  6.  7.  8.  9.]\n",
      " [10. 11. 12. 13. 14. 15. 16. 17. 18. 19.]]\n",
      "mean : (2, 1)\n",
      "variance : (2, 1)\n",
      "beta : (10,)\n",
      "alpha : (10,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 10), dtype=float32, numpy=\n",
       "array([[-1.5661536 , -1.2181195 , -0.87008536, -0.5220512 , -0.17401707,\n",
       "         0.17401707,  0.5220512 ,  0.87008536,  1.2181195 ,  1.5661536 ],\n",
       "       [-1.5661536 , -1.2181195 , -0.87008536, -0.5220512 , -0.17401707,\n",
       "         0.17401707,  0.5220512 ,  0.87008536,  1.2181195 ,  1.5661536 ]],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 230,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l2(arr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 13. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [],
   "source": [
    "## create custom training loops\n",
    "from tensorflow.keras.layers import Dense, Flatten\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.metrics import sparse_categorical_crossentropy, Accuracy, Mean\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras.datasets as tfds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train, y_train), (X_test, y_test) = tfds.fashion_mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_scale = X_train/255.\n",
    "X_test_scale = X_test/255."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 422,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_status(epoch, max_step, step, metrics, metrics_name, loss):\n",
    "    \n",
    "    status = ' - '.join(['{} {:.3f}'.format(metrics_name[i], float(m_result.result())) for i, m_result in enumerate([loss] + metrics)])\n",
    "    end = '' if step < max_step else '\\n'\n",
    "    \n",
    "    print('\\repoch {} {} {}/{}'.format(epoch, status, step, max_step), end=end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 425,
   "metadata": {},
   "outputs": [],
   "source": [
    "## define loss, custom metrics\n",
    "mean_loss = Mean()\n",
    "metrics = [Accuracy()]\n",
    "metrics_name = ['loss', 'acc']\n",
    "\n",
    "epochs = 20\n",
    "batch_size = 32\n",
    "max_step = X_train.shape[0]\n",
    "\n",
    "optim1 = tf.keras.optimizers.Adam()\n",
    "optim2 = tf.keras.optimizers.SGD(lr = 1e-4, momentum=0.9, nesterov=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 420,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential([\n",
    "    Flatten(input_shape = [28, 28]),\n",
    "    Dense(100, activation = 'elu'),\n",
    "    Dense(100, activation = 'elu'),\n",
    "    Dense(10, activation = 'softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 424,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1 loss 1.357 - acc 0.570 60000/60000\n",
      "epoch 2 loss 1.087 - acc 0.651 60000/60000\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(42)\n",
    "tf.keras.backend.clear_session()\n",
    "\n",
    "for epoch in range(1, epochs+1):\n",
    "    \n",
    "    for step in range(X_train.shape[0]//batch_size+1):\n",
    "        batch_idx = np.random.choice(60000,size = batch_size)\n",
    "        X_batch, y_batch = X_train_scale[batch_idx], y_train[batch_idx]\n",
    "        \n",
    "        with tf.GradientTape() as tape:\n",
    "            output = model(X_batch)\n",
    "            main_loss = tf.reduce_mean(sparse_categorical_crossentropy(y_batch, output))\n",
    "            loss = tf.add_n([main_loss], model.losses)\n",
    "        \n",
    "        gradient = tape.gradient(loss, model.trainable_variables)\n",
    "        \n",
    "#         optim1.apply_gradients(zip(gradient[3:], model.trainable_variables[3:]))\n",
    "#         optim2.apply_gradients(zip(gradient[:3], model.trainable_variables[:3]))\n",
    "        optim2.apply_gradients(zip(gradient, model.trainable_variables))\n",
    "        \n",
    "        mean_loss(loss)\n",
    "        \n",
    "        for metric in metrics:\n",
    "            metric(y_batch, tf.argmax(output, axis = 1))\n",
    "        \n",
    "        print_status(epoch, max_step, step*batch_size, metrics, metrics_name, mean_loss)\n",
    "\n",
    "    if epoch == 2:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 445,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=0.71234375>"
      ]
     },
     "execution_count": 445,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics[0](y_test, np.argmax(model.predict(X_test_scale), axis=-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train on adaptive optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 431,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = Sequential([\n",
    "    Flatten(input_shape = [28, 28]),\n",
    "    Dense(100, activation = 'elu'),\n",
    "    Dense(100, activation = 'elu'),\n",
    "    Dense(10, activation = 'softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 433,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1 loss 0.470 - acc 0.832 60000/60000\n",
      "epoch 2 loss 0.419 - acc 0.849 60000/60000\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(42)\n",
    "tf.keras.backend.clear_session()\n",
    "\n",
    "for epoch in range(1, epochs+1):\n",
    "    \n",
    "    for step in range(X_train.shape[0]//batch_size+1):\n",
    "        batch_idx = np.random.choice(60000,size = batch_size)\n",
    "        X_batch, y_batch = X_train_scale[batch_idx], y_train[batch_idx]\n",
    "        \n",
    "        with tf.GradientTape() as tape:\n",
    "            output = model2(X_batch)\n",
    "            main_loss = tf.reduce_mean(sparse_categorical_crossentropy(y_batch, output))\n",
    "            loss = tf.add_n([main_loss], model2.losses)\n",
    "        \n",
    "        gradient = tape.gradient(loss, model2.trainable_variables)\n",
    "        \n",
    "#         optim1.apply_gradients(zip(gradient[3:], model.trainable_variables[3:]))\n",
    "#         optim2.apply_gradients(zip(gradient[:3], model.trainable_variables[:3]))\n",
    "        optim1.apply_gradients(zip(gradient, model2.trainable_variables))\n",
    "        \n",
    "        mean_loss(loss)\n",
    "        \n",
    "        for metric in metrics:\n",
    "            metric(y_batch, tf.argmax(output, axis = 1))\n",
    "        \n",
    "        print_status(epoch, max_step, step*batch_size, metrics, metrics_name, mean_loss)\n",
    "\n",
    "    if epoch == 2:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 444,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=0.72956306>"
      ]
     },
     "execution_count": 444,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics[0](y_test, np.argmax(model2.predict(X_test_scale), axis=-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train top level with adaptive and train low level with momentum SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 436,
   "metadata": {},
   "outputs": [],
   "source": [
    "model3 = Sequential([\n",
    "    Flatten(input_shape = [28, 28]),\n",
    "    Dense(100, activation = 'elu'),\n",
    "    Dense(100, activation = 'elu'),\n",
    "    Dense(10, activation = 'softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 437,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1 loss 0.831 - acc 0.697 60000/60000\n",
      "epoch 2 loss 0.765 - acc 0.722 60000/60000\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(42)\n",
    "tf.keras.backend.clear_session()\n",
    "\n",
    "for epoch in range(1, epochs+1):\n",
    "    \n",
    "    for step in range(X_train.shape[0]//batch_size+1):\n",
    "        batch_idx = np.random.choice(60000,size = batch_size)\n",
    "        X_batch, y_batch = X_train_scale[batch_idx], y_train[batch_idx]\n",
    "        \n",
    "        with tf.GradientTape() as tape:\n",
    "            output = model3(X_batch)\n",
    "            main_loss = tf.reduce_mean(sparse_categorical_crossentropy(y_batch, output))\n",
    "            loss = tf.add_n([main_loss], model3.losses)\n",
    "        \n",
    "        gradient = tape.gradient(loss, model3.trainable_variables)\n",
    "        \n",
    "        optim1.apply_gradients(zip(gradient[3:], model3.trainable_variables[3:]))\n",
    "        optim2.apply_gradients(zip(gradient[:3], model3.trainable_variables[:3]))\n",
    "        \n",
    "        mean_loss(loss)\n",
    "        \n",
    "        for metric in metrics:\n",
    "            metric(y_batch, tf.argmax(output, axis = 1))\n",
    "        \n",
    "        print_status(epoch, max_step, step*batch_size, metrics, metrics_name, mean_loss)\n",
    "\n",
    "    if epoch == 2:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 439,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=0.72514975>"
      ]
     },
     "execution_count": 439,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics[0](y_test, tf.argmax(model3(X_test_scale), axis = 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**It seems that the adam optimizer has the most converge rate**. \n",
    "However, the generalize error seems to be equal with SGD optimizer.  \n",
    "While, the model which train with adam and SGD performance and convergence rate is between Adam and SGD!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
