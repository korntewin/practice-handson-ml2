{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Char level language model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "shakespeare_url = \"http://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('datasets/ch16-nlp/input.txt', <http.client.HTTPMessage at 0x7fc73e462a50>)"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import urllib.request\n",
    "import os\n",
    "import ssl\n",
    "import numpy as np\n",
    "\n",
    "ssl._create_default_https_context = ssl._create_unverified_context\n",
    "\n",
    "os.makedirs('datasets/ch16-nlp', exist_ok = True)\n",
    "urllib.request.urlretrieve(shakespeare_url, 'datasets/ch16-nlp/input.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('datasets/ch16-nlp/input.txt', 'r') as f:\n",
    "    all_lines = f.read()\n",
    "    \n",
    "#     for ind in range(len(all_lines)):\n",
    "#         all_lines[ind] = all_lines[ind].strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1115394\n"
     ]
    }
   ],
   "source": [
    "print(len(all_lines))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(char_level = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.fit_on_texts(all_lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_index = tokenizer.word_index\n",
    "index_word = tokenizer.index_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys([' ', 'e', 't', 'o', 'a', 'i', 'h', 's', 'r', 'n', '\\n', 'l', 'd', 'u', 'm', 'y', 'w', ',', 'c', 'f', 'g', 'b', 'p', ':', 'k', 'v', '.', \"'\", ';', '?', '!', '-', 'j', 'q', 'x', 'z', '3', '&', '$'])\n"
     ]
    }
   ],
   "source": [
    "print(word_index.keys())\n",
    "n_char = len(word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = tokenizer.texts_to_sequences([all_lines[:2]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[20, 6]]\n"
     ]
    }
   ],
   "source": [
    "print(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_texts = all_lines[:int(0.9*len(all_lines))]\n",
    "valid_texts = all_lines[int(0.9*len(all_lines)):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Assume that we never see valid texts before\n",
    "tokenizer.fit_on_texts(train_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "[train_sequences] = np.array(tokenizer.texts_to_sequences([train_texts])) - 1\n",
    "[valid_sequences] = np.array(tokenizer.texts_to_sequences([valid_texts])) - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(texts):\n",
    "    [sequences] = np.array(tokenizer.texts_to_sequences([texts])) - 1\n",
    "    return tf.one_hot(sequences, n_char)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(5, 39), dtype=float32, numpy=\n",
       "array([[0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0.]], dtype=float32)>"
      ]
     },
     "execution_count": 239,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocess('Hello')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxlen = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1003854,)\n"
     ]
    }
   ],
   "source": [
    "print(train_sequences.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_dataset(sequences):\n",
    "    \n",
    "    trainset = tf.data.Dataset.from_tensor_slices(sequences)\n",
    "    trainset = trainset.window(size = maxlen+1, shift = 1, drop_remainder = True)\n",
    "    trainset = trainset.flat_map(lambda x : x.batch(maxlen+1))\n",
    "    trainset = trainset.batch(32)\n",
    "    trainset = trainset.map(lambda x : (x[:, :-1], x[:, 1:]))\n",
    "    trainset = trainset.map(lambda train, target : (tf.one_hot(train, n_char), target))\n",
    "    trainset = trainset.shuffle(100).prefetch(2)\n",
    "    return trainset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset = prepare_dataset(train_sequences)\n",
    "validset = prepare_dataset(valid_sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(<tf.Tensor: shape=(32, 50, 39), dtype=float32, numpy=\n",
      "array([[[0., 0., 0., ..., 0., 0., 0.],\n",
      "        [0., 0., 0., ..., 0., 0., 0.],\n",
      "        [0., 0., 0., ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 1., 0., ..., 0., 0., 0.],\n",
      "        [1., 0., 0., ..., 0., 0., 0.],\n",
      "        [0., 0., 0., ..., 0., 0., 0.]],\n",
      "\n",
      "       [[0., 0., 0., ..., 0., 0., 0.],\n",
      "        [0., 0., 0., ..., 0., 0., 0.],\n",
      "        [0., 0., 0., ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [1., 0., 0., ..., 0., 0., 0.],\n",
      "        [0., 0., 0., ..., 0., 0., 0.],\n",
      "        [0., 0., 0., ..., 0., 0., 0.]],\n",
      "\n",
      "       [[0., 0., 0., ..., 0., 0., 0.],\n",
      "        [0., 0., 0., ..., 0., 0., 0.],\n",
      "        [0., 0., 0., ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0., ..., 0., 0., 0.],\n",
      "        [0., 0., 0., ..., 0., 0., 0.],\n",
      "        [0., 0., 0., ..., 0., 0., 0.]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[0., 0., 0., ..., 0., 0., 0.],\n",
      "        [0., 0., 0., ..., 0., 0., 0.],\n",
      "        [0., 0., 0., ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0., ..., 0., 0., 0.],\n",
      "        [0., 0., 0., ..., 0., 0., 0.],\n",
      "        [0., 0., 0., ..., 0., 0., 0.]],\n",
      "\n",
      "       [[0., 0., 0., ..., 0., 0., 0.],\n",
      "        [0., 0., 0., ..., 0., 0., 0.],\n",
      "        [0., 0., 1., ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0., ..., 0., 0., 0.],\n",
      "        [0., 0., 0., ..., 0., 0., 0.],\n",
      "        [0., 0., 0., ..., 0., 0., 0.]],\n",
      "\n",
      "       [[0., 0., 0., ..., 0., 0., 0.],\n",
      "        [0., 0., 1., ..., 0., 0., 0.],\n",
      "        [0., 0., 0., ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0., ..., 0., 0., 0.],\n",
      "        [0., 0., 0., ..., 0., 0., 0.],\n",
      "        [0., 1., 0., ..., 0., 0., 0.]]], dtype=float32)>, <tf.Tensor: shape=(32, 50), dtype=int64, numpy=\n",
      "array([[17, 10, 15, ...,  0, 14,  3],\n",
      "       [10, 15,  3, ..., 14,  3,  8],\n",
      "       [15,  3, 13, ...,  3,  8,  1],\n",
      "       ...,\n",
      "       [14,  5,  2, ...,  4,  9, 12],\n",
      "       [ 5,  2, 15, ...,  9, 12,  1],\n",
      "       [ 2, 15, 10, ..., 12,  1,  8]])>)\n"
     ]
    }
   ],
   "source": [
    "for data in trainset.take(1):\n",
    "    print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create basic RNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.backend.clear_session()\n",
    "\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Conv1D(32, 2, dilation_rate = 1, padding = 'causal', activation = 'elu', input_shape = [maxlen, n_char]),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Conv1D(48, 2, dilation_rate = 2, padding = 'causal', activation = 'elu'),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Conv1D(64, 2, dilation_rate = 4, padding = 'causal', activation = 'elu'),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Conv1D(96, 2, dilation_rate = 8, padding = 'causal', activation = 'elu'),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.GRU(128, return_sequences = True, dropout=0.2, recurrent_dropout=0.2),\n",
    "    keras.layers.Dense(n_char, activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(loss='sparse_categorical_crossentropy', metrics=['acc'], optimizer = 'nadam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d (Conv1D)              (None, 50, 32)            2528      \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (None, 50, 32)            128       \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 50, 48)            3120      \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 50, 48)            192       \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 50, 64)            6208      \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 50, 64)            256       \n",
      "_________________________________________________________________\n",
      "conv1d_3 (Conv1D)            (None, 50, 96)            12384     \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 50, 96)            384       \n",
      "_________________________________________________________________\n",
      "gru (GRU)                    (None, 50, 128)           86784     \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 50, 39)            5031      \n",
      "=================================================================\n",
      "Total params: 117,015\n",
      "Trainable params: 116,535\n",
      "Non-trainable params: 480\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.fit(trainset, epochs = 5, validation_data=validset)\n",
    "## The model take too long to train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate sequences with naive method\n",
    "def next_char(texts):\n",
    "    next_char_class = model.predict_classes(preprocess(texts))[0][-1]\n",
    "    return next_char_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(0, 39), dtype=float32, numpy=array([], shape=(0, 39), dtype=float32)>"
      ]
     },
     "execution_count": 253,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocess(['hello'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess2(texts):\n",
    "    X = np.array(tokenizer.texts_to_sequences(texts)) - 1\n",
    "    return tf.one_hot(X, n_char)\n",
    "\n",
    "def preprocess(texts):\n",
    "    sequences = np.array(tokenizer.texts_to_sequences([texts])) - 1\n",
    "    return tf.one_hot(sequences, n_char)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* batch with consecutive between batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stateful RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_consecutive_batch(sequences):\n",
    "    batch_size = 32\n",
    "    window_size = maxlen + 1\n",
    "    shift = window_size - 1\n",
    "    \n",
    "    sequences_parts = np.array_split(sequences, batch_size)\n",
    "    datasets = []\n",
    "    \n",
    "    for part in sequences_parts:\n",
    "        \n",
    "        dataset = tf.data.Dataset.from_tensor_slices(part)\n",
    "        dataset = dataset.window(window_size, shift, drop_remainder = True)\n",
    "        dataset = dataset.flat_map(lambda x: x.batch(window_size))\n",
    "        datasets.append(dataset)\n",
    "        \n",
    "    con_dataset = tf.data.Dataset.zip(tuple(datasets)).map(lambda *window: tf.stack(window))\n",
    "    con_dataset = con_dataset.map(lambda x : (x[:, :-1], x[:, 1:]))\n",
    "    con_dataset = con_dataset.map(lambda train, target : (tf.one_hot(train, n_char), target)).prefetch(1)\n",
    "    return con_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set_stateful = create_consecutive_batch(train_sequences)\n",
    "valid_set_stateful = create_consecutive_batch(valid_sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(<tf.Tensor: shape=(32, 50, 39), dtype=float32, numpy=\n",
      "array([[[0., 0., 0., ..., 0., 0., 0.],\n",
      "        [0., 0., 0., ..., 0., 0., 0.],\n",
      "        [0., 0., 0., ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0., ..., 0., 0., 0.],\n",
      "        [0., 0., 1., ..., 0., 0., 0.],\n",
      "        [0., 0., 0., ..., 0., 0., 0.]],\n",
      "\n",
      "       [[0., 0., 1., ..., 0., 0., 0.],\n",
      "        [0., 1., 0., ..., 0., 0., 0.],\n",
      "        [0., 0., 0., ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [1., 0., 0., ..., 0., 0., 0.],\n",
      "        [0., 0., 0., ..., 0., 0., 0.],\n",
      "        [0., 1., 0., ..., 0., 0., 0.]],\n",
      "\n",
      "       [[1., 0., 0., ..., 0., 0., 0.],\n",
      "        [0., 0., 0., ..., 0., 0., 0.],\n",
      "        [0., 1., 0., ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0., ..., 0., 0., 0.],\n",
      "        [0., 1., 0., ..., 0., 0., 0.],\n",
      "        [0., 0., 0., ..., 0., 0., 0.]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[0., 0., 0., ..., 0., 0., 0.],\n",
      "        [0., 1., 0., ..., 0., 0., 0.],\n",
      "        [0., 0., 0., ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 1., 0., ..., 0., 0., 0.],\n",
      "        [1., 0., 0., ..., 0., 0., 0.],\n",
      "        [0., 0., 0., ..., 0., 0., 0.]],\n",
      "\n",
      "       [[0., 0., 0., ..., 0., 0., 0.],\n",
      "        [0., 0., 0., ..., 0., 0., 0.],\n",
      "        [0., 0., 0., ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0., ..., 0., 0., 0.],\n",
      "        [0., 1., 0., ..., 0., 0., 0.],\n",
      "        [0., 0., 0., ..., 0., 0., 0.]],\n",
      "\n",
      "       [[0., 0., 0., ..., 0., 0., 0.],\n",
      "        [0., 0., 0., ..., 0., 0., 0.],\n",
      "        [0., 0., 0., ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0., ..., 0., 0., 0.],\n",
      "        [0., 0., 0., ..., 0., 0., 0.],\n",
      "        [0., 0., 0., ..., 0., 0., 0.]]], dtype=float32)>, <tf.Tensor: shape=(32, 50), dtype=int64, numpy=\n",
      "array([[10, 10, 20, ...,  2,  5,  7],\n",
      "       [ 1,  8, 27, ..., 20,  1,  2],\n",
      "       [21,  1, 11, ...,  1, 26, 10],\n",
      "       ...,\n",
      "       [ 1, 30,  0, ...,  0, 22,  5],\n",
      "       [ 4,  8, 14, ...,  1,  4, 17],\n",
      "       [26, 10, 10, ..., 26, 10, 10]])>)\n",
      "(<tf.Tensor: shape=(32, 50, 39), dtype=float32, numpy=\n",
      "array([[[0., 0., 0., ..., 0., 0., 0.],\n",
      "        [0., 0., 1., ..., 0., 0., 0.],\n",
      "        [0., 0., 0., ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0., ..., 0., 0., 0.],\n",
      "        [0., 0., 0., ..., 0., 0., 0.],\n",
      "        [1., 0., 0., ..., 0., 0., 0.]],\n",
      "\n",
      "       [[0., 0., 1., ..., 0., 0., 0.],\n",
      "        [1., 0., 0., ..., 0., 0., 0.],\n",
      "        [0., 0., 0., ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [1., 0., 0., ..., 0., 0., 0.],\n",
      "        [0., 0., 0., ..., 0., 0., 0.],\n",
      "        [0., 0., 0., ..., 0., 0., 0.]],\n",
      "\n",
      "       [[0., 0., 0., ..., 0., 0., 0.],\n",
      "        [0., 0., 0., ..., 0., 0., 0.],\n",
      "        [0., 0., 0., ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0., ..., 0., 0., 0.],\n",
      "        [0., 0., 0., ..., 0., 0., 0.],\n",
      "        [0., 0., 0., ..., 0., 0., 0.]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[0., 0., 0., ..., 0., 0., 0.],\n",
      "        [0., 0., 1., ..., 0., 0., 0.],\n",
      "        [0., 0., 0., ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 1., 0., ..., 0., 0., 0.],\n",
      "        [1., 0., 0., ..., 0., 0., 0.],\n",
      "        [0., 0., 0., ..., 0., 0., 0.]],\n",
      "\n",
      "       [[0., 0., 0., ..., 0., 0., 0.],\n",
      "        [1., 0., 0., ..., 0., 0., 0.],\n",
      "        [0., 0., 0., ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0., ..., 0., 0., 0.],\n",
      "        [0., 1., 0., ..., 0., 0., 0.],\n",
      "        [0., 0., 0., ..., 0., 0., 0.]],\n",
      "\n",
      "       [[0., 0., 0., ..., 0., 0., 0.],\n",
      "        [0., 0., 0., ..., 0., 0., 0.],\n",
      "        [0., 0., 0., ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0., ..., 0., 0., 0.],\n",
      "        [0., 0., 0., ..., 0., 0., 0.],\n",
      "        [0., 0., 0., ..., 0., 0., 0.]]], dtype=float32)>, <tf.Tensor: shape=(32, 50), dtype=int64, numpy=\n",
      "array([[ 2,  4, 23, ..., 17,  0, 20],\n",
      "       [ 0, 15,  3, ..., 16,  5,  2],\n",
      "       [10, 24,  4, ...,  3, 25,  1],\n",
      "       ...,\n",
      "       [ 2, 15, 28, ...,  0, 16,  3],\n",
      "       [ 0,  6,  3, ...,  1,  7, 17],\n",
      "       [20,  3,  9, ..., 17, 31, 31]])>)\n"
     ]
    }
   ],
   "source": [
    "for data in valid_set_stateful.take(2):\n",
    "    print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 404,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.backend.clear_session()\n",
    "\n",
    "model_stateful = keras.models.Sequential([\n",
    "    keras.layers.Conv1D(32, 2, dilation_rate = 1, padding = 'causal', activation = 'elu', batch_input_shape = [32, maxlen, n_char]),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Conv1D(48, 2, dilation_rate = 2, padding = 'causal', activation = 'elu'),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Conv1D(64, 2, dilation_rate = 4, padding = 'causal', activation = 'elu'),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Conv1D(96, 2, dilation_rate = 8, padding = 'causal', activation = 'elu'),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.GRU(128, return_sequences = True, dropout=0.2, \n",
    "                     recurrent_dropout=0.2, stateful=True),\n",
    "    keras.layers.Dense(n_char, activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(loss='sparse_categorical_crossentropy', metrics=['acc'], optimizer = 'nadam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResetStateCallback(keras.callbacks.Callback):\n",
    "    def on_epoch_begin(self, epoch, logs):\n",
    "        self.model.reset_states()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "627/627 [==============================] - 89s 141ms/step - loss: 2.0342 - acc: 0.4027 - val_loss: 1.9117 - val_acc: 0.4256\n",
      "Epoch 2/5\n",
      "627/627 [==============================] - 72s 115ms/step - loss: 1.7933 - acc: 0.4640 - val_loss: 1.8190 - val_acc: 0.4505\n",
      "Epoch 3/5\n",
      "627/627 [==============================] - 81s 130ms/step - loss: 1.7202 - acc: 0.4815 - val_loss: 1.7741 - val_acc: 0.4626\n",
      "Epoch 4/5\n",
      "627/627 [==============================] - 71s 113ms/step - loss: 1.6817 - acc: 0.4908 - val_loss: 1.7427 - val_acc: 0.4718\n",
      "Epoch 5/5\n",
      "627/627 [==============================] - 72s 114ms/step - loss: 1.6571 - acc: 0.4970 - val_loss: 1.7238 - val_acc: 0.4775\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fc6e7155d50>"
      ]
     },
     "execution_count": 406,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_set_stateful, epochs = 5, validation_data=valid_set_stateful, callbacks = [ResetStateCallback()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 407,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.keras as keras\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 411,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb.npz\n",
      "17465344/17464789 [==============================] - 1s 0us/step\n"
     ]
    }
   ],
   "source": [
    "(x_train, y_train), (x_test, y_test) = keras.datasets.imdb.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 413,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25000,)\n",
      "[1, 14, 22, 16, 43, 530, 973, 1622, 1385, 65, 458, 4468, 66, 3941, 4, 173, 36, 256, 5, 25, 100, 43, 838, 112, 50, 670, 22665, 9, 35, 480, 284, 5, 150, 4, 172, 112, 167, 21631, 336, 385, 39, 4, 172, 4536, 1111, 17, 546, 38, 13, 447, 4, 192, 50, 16, 6, 147, 2025, 19, 14, 22, 4, 1920, 4613, 469, 4, 22, 71, 87, 12, 16, 43, 530, 38, 76, 15, 13, 1247, 4, 22, 17, 515, 17, 12, 16, 626, 18, 19193, 5, 62, 386, 12, 8, 316, 8, 106, 5, 4, 2223, 5244, 16, 480, 66, 3785, 33, 4, 130, 12, 16, 38, 619, 5, 25, 124, 51, 36, 135, 48, 25, 1415, 33, 6, 22, 12, 215, 28, 77, 52, 5, 14, 407, 16, 82, 10311, 8, 4, 107, 117, 5952, 15, 256, 4, 31050, 7, 3766, 5, 723, 36, 71, 43, 530, 476, 26, 400, 317, 46, 7, 4, 12118, 1029, 13, 104, 88, 4, 381, 15, 297, 98, 32, 2071, 56, 26, 141, 6, 194, 7486, 18, 4, 226, 22, 21, 134, 476, 26, 480, 5, 144, 30, 5535, 18, 51, 36, 28, 224, 92, 25, 104, 4, 226, 65, 16, 38, 1334, 88, 12, 16, 283, 5, 16, 4472, 113, 103, 32, 15, 16, 5345, 19, 178, 32]\n"
     ]
    }
   ],
   "source": [
    "print(x_train.shape)\n",
    "print(x_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 414,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb_word_index.json\n",
      "1646592/1641221 [==============================] - 0s 0us/step\n"
     ]
    }
   ],
   "source": [
    "word_index = keras.datasets.imdb.get_word_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 426,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"<sos> this film was just brilliant casting location scenery story direction everyone's really suited the part they played and you could just imagine being there robert redford's is an amazing actor and now the same being director norman's father came from the same scottish island as myself so i loved the fact there was a real connection with this film the witty remarks throughout the film were great it was just brilliant so much that i bought the film as soon as it was released for retail and would recommend it to everyone to watch and the fly fishing was amazing really cried at the end it was so sad and you know what they say if you cry at a film it must have been good and this definitely was also congratulations to the two little boy's that played the part's of norman and paul they were just brilliant children are often left out of the praising list i think because the stars that play them all grown up are such a big profile for the whole film but these children are amazing and should be praised for what they have done don't you think the whole story was so lovely because it was true and was someone's life after all that was shared with us all\""
      ]
     },
     "execution_count": 426,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index_word = {id_ + 3: word for word, id_ in word_index.items()}\n",
    "\n",
    "for id_, word in enumerate(['<pad>', '<sos>', '<unk>']):\n",
    "    index_word[id_] = word\n",
    "    \n",
    "\" \".join([index_word[id_] for id_ in x_train[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 428,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mDownloading and preparing dataset imdb_reviews/plain_text/1.0.0 (download: 80.23 MiB, generated: Unknown size, total: 80.23 MiB) to /Users/boonchuay/tensorflow_datasets/imdb_reviews/plain_text/1.0.0...\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "08f3cf91c12e4ce2888b098b547bfe5e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Dl Completed...'), FloatProgress(value=1.0, bar_style='info', layout=Layout(width='…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a94339d55e34b219613fddb9c45fd76",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Dl Size...'), FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shuffling and writing examples to /Users/boonchuay/tensorflow_datasets/imdb_reviews/plain_text/1.0.0.incompleteXHZJOY/imdb_reviews-train.tfrecord\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "97180eae466a48fb96fb8296928051e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=25000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shuffling and writing examples to /Users/boonchuay/tensorflow_datasets/imdb_reviews/plain_text/1.0.0.incompleteXHZJOY/imdb_reviews-test.tfrecord\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "18d18915b8674b9d954b5572b41b9b16",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=25000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shuffling and writing examples to /Users/boonchuay/tensorflow_datasets/imdb_reviews/plain_text/1.0.0.incompleteXHZJOY/imdb_reviews-unsupervised.tfrecord\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "80bb429cf7fd4d8eb7f80693c1e89fa2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=50000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mDataset imdb_reviews downloaded and prepared to /Users/boonchuay/tensorflow_datasets/imdb_reviews/plain_text/1.0.0. Subsequent calls will reuse this data.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import tensorflow_datasets as tfds\n",
    "\n",
    "datasets, info = tfds.load(\"imdb_reviews\", as_supervised=True, with_info=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 429,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['test', 'train', 'unsupervised'])"
      ]
     },
     "execution_count": 429,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasets.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 434,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(<tf.Tensor: shape=(2,), dtype=string, numpy=\n",
      "array([b\"This was an absolutely terrible movie. Don't be lured in by Christopher Walken or Michael Ironside. Both are great actors, but this must simply be their worst role in history. Even their great acting could not redeem this movie's ridiculous storyline. This movie is an early nineties US propaganda piece. The most pathetic scenes were those when the Columbian rebels were making their cases for revolutions. Maria Conchita Alonso appeared phony, and her pseudo-love affair with Walken was nothing but a pathetic emotional plug in a movie that was devoid of any real meaning. I am disappointed that there are movies like this, ruining actor's like Christopher Walken's good name. I could barely sit through it.\",\n",
      "       b'I have been known to fall asleep during films, but this is usually due to a combination of things including, really tired, being warm and comfortable on the sette and having just eaten a lot. However on this occasion I fell asleep because the film was rubbish. The plot development was constant. Constantly slow and boring. Things seemed to happen, but with no explanation of what was causing them or why. I admit, I may have missed part of the film, but i watched the majority of it and everything just seemed to happen of its own accord without any real concern for anything else. I cant recommend this film at all.'],\n",
      "      dtype=object)>, <tf.Tensor: shape=(2,), dtype=int64, numpy=array([0, 0])>)\n"
     ]
    }
   ],
   "source": [
    "for data in datasets['train'].batch(2).take(1):\n",
    "    print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 480,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(x, y):\n",
    "    x = tf.strings.substr(x, 0, 300)\n",
    "    X_batch = tf.strings.regex_replace(x, rb\"<br\\s*/?>\", b\" \")\n",
    "    X_batch = tf.strings.regex_replace(x, b\"[^a-zA-Z']\", b\" \")\n",
    "    x = tf.strings.split(x)\n",
    "    return x.to_tensor(default_value = '<pad>'), y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 481,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(<tf.Tensor: shape=(2, 53), dtype=string, numpy=\n",
      "array([[b'This', b'was', b'an', b'absolutely', b'terrible', b'movie.',\n",
      "        b\"Don't\", b'be', b'lured', b'in', b'by', b'Christopher',\n",
      "        b'Walken', b'or', b'Michael', b'Ironside.', b'Both', b'are',\n",
      "        b'great', b'actors,', b'but', b'this', b'must', b'simply', b'be',\n",
      "        b'their', b'worst', b'role', b'in', b'history.', b'Even',\n",
      "        b'their', b'great', b'acting', b'could', b'not', b'redeem',\n",
      "        b'this', b\"movie's\", b'ridiculous', b'storyline.', b'This',\n",
      "        b'movie', b'is', b'an', b'early', b'nineties', b'US',\n",
      "        b'propaganda', b'pi', b'<pad>', b'<pad>', b'<pad>'],\n",
      "       [b'I', b'have', b'been', b'known', b'to', b'fall', b'asleep',\n",
      "        b'during', b'films,', b'but', b'this', b'is', b'usually', b'due',\n",
      "        b'to', b'a', b'combination', b'of', b'things', b'including,',\n",
      "        b'really', b'tired,', b'being', b'warm', b'and', b'comfortable',\n",
      "        b'on', b'the', b'sette', b'and', b'having', b'just', b'eaten',\n",
      "        b'a', b'lot.', b'However', b'on', b'this', b'occasion', b'I',\n",
      "        b'fell', b'asleep', b'because', b'the', b'film', b'was',\n",
      "        b'rubbish.', b'The', b'plot', b'development', b'was',\n",
      "        b'constant.', b'Cons']], dtype=object)>, <tf.Tensor: shape=(2,), dtype=int64, numpy=array([0, 0])>)\n"
     ]
    }
   ],
   "source": [
    "for data in datasets['train'].batch(2).map(preprocess).take(1):\n",
    "    print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 482,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "counter = Counter()\n",
    "for data in datasets['train'].batch(32).map(preprocess):\n",
    "    for review in data[0]:\n",
    "        counter.update(review.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We will create tokenizer pipeline by ourselves!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 501,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 10000\n",
    "num_oov_bucket = 1000\n",
    "\n",
    "my_word_ind = {id_: word[0] for id_, word in enumerate(counter.most_common(vocab_size))}\n",
    "my_ind_word = {value: key for key, value in my_word_ind.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 503,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = [word[0] for id_, word in enumerate(counter.most_common(vocab_size))]\n",
    "indices = tf.range(len(words), dtype = tf.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 504,
   "metadata": {},
   "outputs": [],
   "source": [
    "table = tf.lookup.KeyValueTensorInitializer(words, indices)\n",
    "table = tf.lookup.StaticVocabularyTable(table, num_oov_bucket)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 510,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=int64, numpy=10449>"
      ]
     },
     "execution_count": 510,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table.lookup(tf.constant('sadrussia'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 534,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(string_tensor):\n",
    "    return table.lookup(string_tensor)\n",
    "def tokenize_xybatch(string_tensor, target):\n",
    "    return table.lookup(string_tensor), target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create simple model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 523,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.backend.clear_session()\n",
    "\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Lambda(lambda x : tokenize(x), input_shape = [None], dtype=tf.string),\n",
    "    keras.layers.Embedding(vocab_size+num_oov_bucket, 50, mask_zero = True),\n",
    "    keras.layers.Conv1D(32, 3, dilation_rate = 1, activation = 'relu', padding='causal'),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Conv1D(48, 3, dilation_rate = 2, activation = 'relu', padding='causal'),\n",
    "    keras.layers.BatchNormalization(),   \n",
    "    keras.layers.Conv1D(64, 3, dilation_rate = 4, activation = 'relu', padding='causal'),\n",
    "    keras.layers.BatchNormalization(), \n",
    "    keras.layers.Conv1D(80, 3, dilation_rate = 8, activation = 'relu', padding='causal'),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.GRU(128, dropout=0.2, recurrent_dropout = 0.2),\n",
    "    keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model.compile(loss = 'binary_crossentropy', optimizer = 'nadam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 524,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lambda (Lambda)              (None, None)              0         \n",
      "_________________________________________________________________\n",
      "embedding (Embedding)        (None, None, 50)          550000    \n",
      "_________________________________________________________________\n",
      "conv1d (Conv1D)              (None, None, 32)          4832      \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (None, None, 32)          128       \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, None, 48)          4656      \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, None, 48)          192       \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, None, 64)          9280      \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, None, 64)          256       \n",
      "_________________________________________________________________\n",
      "conv1d_3 (Conv1D)            (None, None, 80)          15440     \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, None, 80)          320       \n",
      "_________________________________________________________________\n",
      "gru (GRU)                    (None, 128)               80640     \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 665,873\n",
      "Trainable params: 665,425\n",
      "Non-trainable params: 448\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 517,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([ 2 53], shape=(2,), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "for data in datasets['train'].batch(2).map(preprocess).take(1):\n",
    "    print(tf.shape(data[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 527,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "782/782 [==============================] - 158s 202ms/step - loss: 0.6611 - accuracy: 0.5901 - val_loss: 0.6198 - val_accuracy: 0.6455\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fc701ae8490>"
      ]
     },
     "execution_count": 527,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(datasets['train'].batch(32).map(preprocess), validation_data=datasets['test'].batch(32).map(preprocess))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 528,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_hub as hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 529,
   "metadata": {},
   "outputs": [],
   "source": [
    "TFHUB_CACHE_DIR = os.path.join(os.curdir, \"datasets/ch16-nlp/my_tfhub_cache\")\n",
    "os.environ[\"TFHUB_CACHE_DIR\"] = TFHUB_CACHE_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 530,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.backend.clear_session()\n",
    "\n",
    "model = keras.Sequential([\n",
    "    hub.KerasLayer(\"https://tfhub.dev/google/tf2-preview/nnlm-en-dim50/1\",\n",
    "                   dtype=tf.string, input_shape=[], output_shape=[50]),\n",
    "    keras.layers.Dense(128, activation=\"relu\"),\n",
    "    keras.layers.Dense(1, activation=\"sigmoid\")\n",
    "])\n",
    "model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\",\n",
    "              metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 570,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./datasets/ch16-nlp/my_tfhub_cache/82c4aaf4250ffb09088bd48368ee7fd00e5464fe.descriptor.txt\n",
      "./datasets/ch16-nlp/my_tfhub_cache/82c4aaf4250ffb09088bd48368ee7fd00e5464fe/saved_model.pb\n",
      "./datasets/ch16-nlp/my_tfhub_cache/82c4aaf4250ffb09088bd48368ee7fd00e5464fe/variables/variables.data-00000-of-00001\n",
      "./datasets/ch16-nlp/my_tfhub_cache/82c4aaf4250ffb09088bd48368ee7fd00e5464fe/variables/variables.index\n",
      "./datasets/ch16-nlp/my_tfhub_cache/82c4aaf4250ffb09088bd48368ee7fd00e5464fe/assets/tokens.txt\n"
     ]
    }
   ],
   "source": [
    "for dirpath, dirnames, filenames in os.walk(TFHUB_CACHE_DIR):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirpath, filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 571,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "keras_layer (KerasLayer)     (None, 50)                48190600  \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 128)               6528      \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 48,197,257\n",
      "Trainable params: 6,657\n",
      "Non-trainable params: 48,190,600\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Model in tensorhub is created with TF2.0, which incompatible with TF2.3!**\n",
    "\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "datasets, info = tfds.load(\"imdb_reviews\", as_supervised=True, with_info=True)\n",
    "train_size = info.splits[\"train\"].num_examples\n",
    "batch_size = 32\n",
    "train_set = datasets[\"train\"].repeat().batch(batch_size).prefetch(1)\n",
    "history = model.fit(train_set, steps_per_epoch=train_size // batch_size, epochs=5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Automatic Translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 580,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import tensorflow_addons as tfa\n",
    "\n",
    "\n",
    "encoder_inputs = keras.layers.Input(shape=[None], dtype=tf.int32)\n",
    "decoder_inputs = keras.layers.Input(shape=[None], dtype=tf.int32)\n",
    "sequence_length_inputs = keras.layers.Input(shape=[], dtype=tf.int32)\n",
    "\n",
    "embedding_layer = keras.layers.Embedding(vocab_size, 128)\n",
    "encoder_embeddings = embedding_layer(encoder_inputs)\n",
    "decoder_embeddings = embedding_layer(decoder_inputs)\n",
    "\n",
    "encoder = keras.layers.LSTM(128, return_state = True)\n",
    "encoder_output, state_h, state_c = encoder(encoder_embeddings)\n",
    "\n",
    "decoder_cell = keras.layers.LSTMCell(128)\n",
    "sampler = tfa.seq2seq.TrainingSampler()\n",
    "output_layer = keras.layers.Dense(vocab_size)\n",
    "decoder = tfa.seq2seq.basic_decoder.BasicDecoder(cell=decoder_cell, sampler = sampler,\n",
    "                                                output_layer = output_layer)\n",
    "\n",
    "final_outputs, final_state, final_sequence_lengths = decoder(decoder_embeddings, \n",
    "                         initial_state = [state_h, state_c], \n",
    "                         sequence_length=sequence_length_inputs)\n",
    "\n",
    "Y_proba = tf.nn.softmax(final_outputs.rnn_output)\n",
    "\n",
    "model = keras.models.Model(inputs=[encoder_inputs, decoder_inputs, sequence_length_inputs], outputs=Y_proba)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 611,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_3\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_23 (InputLayer)           [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_22 (InputLayer)           [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_7 (Embedding)         (None, None, 128)    1280000     input_22[0][0]                   \n",
      "                                                                 input_23[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lstm_7 (LSTM)                   [(None, 128), (None, 131584      embedding_7[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "input_24 (InputLayer)           [(None,)]            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "basic_decoder_7 (BasicDecoder)  (BasicDecoderOutput( 1421584     embedding_7[1][0]                \n",
      "                                                                 lstm_7[0][1]                     \n",
      "                                                                 lstm_7[0][2]                     \n",
      "                                                                 input_24[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Softmax_2 (TensorFl [(None, None, 10000) 0           basic_decoder_7[0][0]            \n",
      "==================================================================================================\n",
      "Total params: 2,833,168\n",
      "Trainable params: 2,833,168\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 612,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.random.randint(vocab_size, size=(1000, 20))\n",
    "y = np.random.randint(vocab_size, size=(1000, 30))\n",
    "seq_len = np.full([1000], 30)\n",
    "\n",
    "y_decoder = np.c_[np.zeros((1000)), y[:, :-1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 613,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 20)\n",
      "(1000, 30)\n",
      "(1000,)\n",
      "(1000, 30)\n"
     ]
    }
   ],
   "source": [
    "print(x.shape)\n",
    "print(y.shape)\n",
    "print(seq_len.shape)\n",
    "print(y_decoder.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 614,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 13s 402ms/step - loss: 9.2103 - acc: 1.0000e-04\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fc6c7948990>"
      ]
     },
     "execution_count": 614,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit([x, y_decoder, seq_len],y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
