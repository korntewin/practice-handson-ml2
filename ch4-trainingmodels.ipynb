{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python_defaultSpec_1599452154712",
   "display_name": "Python 3.8.5 64-bit ('handson_ml': venv)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 4 exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1 . \n",
    "Gradient descent, since its time complexity doesn't depend on number of features, but depend on number of samples.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2 . \n",
    "Stochastic and mini-batch gradient descent, bacuse they are depend only small number of samples and may sway too much to reach the optimum value.  \n",
    "Thus we should scale them first."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3 . \n",
    "With learning rate schedule, maybe yes, because the learning rate may decay so that the gradient is vanished.  \n",
    "Without learning rate schedule, no, but we have to wait long enough be cause the logistic regression is convex problems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 4 . \n",
    "For convex problem, yes, but for another types of problem Gradient descent may stuck in different local minima."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 5 . \n",
    "Validation loss increase over epoch notify that the model is overfitting.  \n",
    "To address ovefitting problem there are 3 main ways as follow:  \n",
    "1. Collect more data so that the the traingset is cover all of types of input existing in validation set  \n",
    "2. Regularization the model  \n",
    "3. Remove sampline noise, because some model may fit to these noise  \n",
    "4. Early stopping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 6 . \n",
    "No, we should further train the model to make sure that this is the point of saturation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 7 . \n",
    "There is no best model, it depends on the situation.  \n",
    "If the number of samples is not that much, we should use batch because the direction toward optimal value is not much. But if the number of samples is too large, calculating the gradient from all samples will not be trivial anymore, we may need mini-batch gradient descent instead."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 8 . \n",
    "Overfitting problems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 9 . \n",
    "underfitting problems is high bias and low variance . \n",
    "We should reduce regularization parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 10 . \n",
    "1. When the model is overfitting.  \n",
    "2. When we suspect that some of the features may be useless.  \n",
    "3. When there are large number of features or the features are correlated to each other."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 11 . \n",
    "Two logistic regressions, because softmax is multilabel not multioutput"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}